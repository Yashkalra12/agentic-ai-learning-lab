{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b9cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document Structure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e430a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Yash Kalra', 'date_created': '2026-01-12'}, page_content='Hello world')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= Document(\n",
    "    page_content=\"Hello world\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Yash Kalra\",\n",
    "        \"date_created\": \"2026-01-12\",\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0731b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)\n",
    "\n",
    "with open(\"../data/text_files/example.txt\", \"w\") as f:\n",
    "    f.write(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69115685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74204684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c82ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.'),\n",
       " Document(metadata={'source': '../data/text_files/example.txt'}, page_content='Hello, world!'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "327fe46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 0}, page_content='embeddings_explained.md\\n2026-01-12\\n1 / 8\\nEmbeddings Explained\\nWhat are Embeddings?\\nEmbeddings are numerical representations of text, images, or other data that capture semantic meaning in\\na high-dimensional vector space. They transform discrete, symbolic data (like words or sentences) into\\ncontinuous numerical vectors that machine learning models can process and understand.\\nThe Core Concept\\nThink of embeddings as a way to translate human language into a \"language\" that computers can\\nunderstand mathematically. Words or phrases with similar meanings are represented by vectors that are\\nclose together in the embedding space.\\nSimple Analogy\\nImagine plotting words on a map:\\nWords with similar meanings are neighbors\\nWords with opposite meanings are far apart\\nRelated concepts form clusters\\nThe \"distance\" between words represents their semantic relationship\\nWhy Embeddings Matter\\n-. Semantic Understanding: Capture meaning, not just spelling\\n/. Similarity Measurement: Enable finding similar content\\n0. Machine Learning Compatibility: Convert text to numbers for ML models\\n1. Dimensionality: Reduce complex data to manageable vectors\\n3. Transfer Learning: Pre-trained embeddings work across tasks\\nHow Embeddings Work\\nWord Embeddings\\nTraditional Approach (Word2Vec, GloVe)\\nEach word gets a fixed vector representation\\nVectors capture relationships: \"king\" - \"man\" + \"woman\" ≈ \"queen\"\\nContext-independent (same word always has same vector)\\nModern Approach (Contextual Embeddings)\\nSame word can have different vectors based on context\\n\"Bank\" (financial) vs \"Bank\" (river) have different embeddings\\nCaptures polysemy (words with multiple meanings)\\nSentence/Document Embeddings'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 1}, page_content=\"embeddings_explained.md\\n2026-01-12\\n2 / 8\\nCombine word embeddings to represent entire sentences or documents\\nMethods include:\\nAveraging word embeddings\\nUsing special tokens (like [CLS] in BERT)\\nTransformer-based encoders\\nTypes of Embedding Models\\n1. Static Embeddings\\nWord2Vec (2013)\\nSkip-gram or CBOW architecture\\nFixed vectors per word\\nFast and lightweight\\nExamples: Google News vectors (300 dimensions)\\nGloVe (2014)\\nGlobal Vectors for Word Representation\\nCombines global and local statistics\\nGood for word analogy tasks\\n2. Contextual Embeddings\\nELMo (2018)\\nEmbeddings from Language Models\\nBidirectional LSTM\\nContext-dependent representations\\nBERT (2018)\\nBidirectional Encoder Representations from Transformers\\nTransformer architecture\\nPre-trained on masked language modeling\\nContext-aware word embeddings\\nRoBERTa, ALBERT, DistilBERT\\nImprovements and variations of BERT\\nBetter performance or efficiency\\n3. Sentence Embeddings\\nUniversal Sentence Encoder\\nGoogle's model for sentence-level embeddings\\nGood for semantic similarity tasks\\nSentence-BERT (SBERT)\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 2}, page_content='embeddings_explained.md\\n2026-01-12\\n3 / 8\\nFine-tuned BERT for sentence embeddings\\nEfficient similarity computation\\nPopular for semantic search\\nInstructor\\nInstruction-tuned embeddings\\nCan be customized with instructions\\nState-of-the-art performance\\nOpenAI Embeddings (text-embedding-ada-002, text-embedding-3)\\nHigh-quality general-purpose embeddings\\nWidely used in production systems\\nEmbedding Dimensions\\nEmbeddings are typically represented as vectors with a fixed number of dimensions:\\nLow-dimensional (50-100): Fast, less expressive\\nMedium-dimensional (200-300): Good balance (Word2Vec, GloVe)\\nHigh-dimensional (384-1536): More expressive, captures nuances\\nBERT: 768 dimensions\\nOpenAI ada-002: 1536 dimensions\\nMany modern models: 384-1024 dimensions\\nProperties of Good Embeddings\\n1. Semantic Similarity\\nSimilar meanings → close vectors\\nMeasured by cosine similarity or Euclidean distance\\n2. Linear Relationships\\nWord analogies work: \"man\" is to \"woman\" as \"king\" is to \"queen\"\\nVector arithmetic: king - man + woman ≈ queen\\n3. Clustering\\nRelated concepts form clusters\\nTopics can be identified through clustering\\n4. Generalization\\nWork across different tasks\\nTransfer learning capabilities\\nHow Embeddings are Created\\nTraining Process'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 3}, page_content='embeddings_explained.md\\n2026-01-12\\n4 / 8\\n-. Data Collection: Large text corpora (Wikipedia, books, web pages)\\n/. Preprocessing:\\nTokenization\\nLowercasing (sometimes)\\nSpecial token handling\\n0. Model Training:\\nPredictive Models: Predict context words (Word2Vec)\\nMasked Language Models: Predict masked words (BERT)\\nContrastive Learning: Learn by comparing similar/dissimilar pairs\\n1. Optimization:\\nMinimize prediction errors\\nMaximize similarity for related items\\nMinimize similarity for unrelated items\\nSelf-Supervised Learning\\nMost embedding models use self-supervised learning:\\nNo manual labeling required\\nLearn from text structure itself\\nPredict words from context or vice versa\\nMeasuring Embedding Similarity\\nCosine Similarity (Most Common)\\nsimilarity = (A · B) / (||A|| × ||B||) \\nMeasures angle between vectors\\nRange: -1 to 1 (typically 0 to 1 for normalized embeddings)\\n1 = identical, 0 = orthogonal, -1 = opposite\\nEuclidean Distance\\ndistance = √Σ(Aᵢ - Bᵢ)² \\nMeasures straight-line distance\\nSmaller distance = more similar\\nDot Product\\nSimple multiplication of corresponding elements'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 4}, page_content='embeddings_explained.md\\n2026-01-12\\n5 / 8\\nFaster computation\\nWorks well with normalized embeddings\\nApplications of Embeddings\\n1. Semantic Search\\nFind documents similar to a query\\nPower search engines and recommendation systems\\n2. RAG (Retrieval-Augmented Generation)\\nConvert documents to embeddings\\nRetrieve relevant chunks for LLM context\\n3. Clustering and Classification\\nGroup similar documents\\nClassify text into categories\\n4. Recommendation Systems\\nFind similar items or users\\nContent-based recommendations\\n5. Anomaly Detection\\nIdentify outliers in embedding space\\nDetect unusual patterns\\n6. Machine Translation\\nMap between languages in shared space\\nCross-lingual understanding\\n7. Sentiment Analysis\\nPositive/negative sentiment clustering\\nEmotion detection\\nEmbeddings in Practice\\nChoosing an Embedding Model\\nConsiderations:\\n-. Task Type: Classification, search, similarity?\\n/. Language: English-only or multilingual?\\n0. Domain: General or specialized (medical, legal, etc.)?\\n1. Speed Requirements: Real-time or batch processing?\\n3. Accuracy Needs: State-of-the-art or good enough?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 5}, page_content='embeddings_explained.md\\n2026-01-12\\n6 / 8\\n4. Resource Constraints: GPU available? Memory limits?\\nPopular Embedding Models (2024)\\nOpen Source:\\nsentence-transformers: Easy-to-use library with many models\\nInstructor: Instruction-tuned, highly customizable\\nBGE (BAAI General Embedding): Strong performance\\nE5: Multilingual embeddings\\nCommercial:\\nOpenAI Embeddings: High quality, widely used\\nCohere Embeddings: Strong multilingual support\\nVoyage AI: Optimized for specific use cases\\nImplementation Example\\n# Using sentence-transformers\\nfrom sentence_transformers import SentenceTransformer \\n \\nmodel = SentenceTransformer(\\'all-MiniLM-L6-v2\\') \\n \\n# Generate embeddings \\nsentences = [\"This is a sample sentence\", \"Another example\"] \\nembeddings = model.encode(sentences) \\n \\n# Compute similarity\\nfrom sklearn.metrics.pairwise import cosine_similarity \\nsimilarity = cosine_similarity([embeddings[0]], [embeddings[1]]) \\nEmbedding Quality Factors\\n1. Training Data\\nSize: Larger corpora generally better\\nQuality: Clean, diverse data\\nDomain: Match your use case\\n2. Model Architecture\\nTransformer-based models generally superior\\nAttention mechanisms capture context\\nBidirectional models understand context better\\n3. Training Objectives\\nTask-specific fine-tuning improves performance\\nContrastive learning for similarity tasks'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 6}, page_content=\"embeddings_explained.md\\n2026-01-12\\n7 / 8\\nMulti-task learning for generalization\\n4. Dimensionality\\nMore dimensions = more capacity\\nBut also more computation and storage\\nDiminishing returns after certain point\\nChallenges and Limitations\\n1. Out-of-Vocabulary Words\\nUnknown words get random or average embeddings\\nSubword tokenization helps (WordPiece, BPE)\\n2. Multilingual Support\\nNot all models work well across languages\\nNeed multilingual training data\\n3. Domain Mismatch\\nGeneral embeddings may not work for specialized domains\\nFine-tuning or domain-specific models needed\\n4. Bias\\nEmbeddings can encode societal biases\\nGender, racial, cultural biases in training data\\nActive area of research\\n5. Computational Cost\\nLarge models require significant resources\\nReal-time applications need optimization\\nBest Practices\\n-. Choose the Right Model: Match model to your task and domain\\n/. Normalize Embeddings: For cosine similarity, normalize vectors\\n0. Batch Processing: Process multiple items together for efficiency\\n1. Caching: Store embeddings to avoid recomputation\\n3. Evaluation: Test on your specific use case, not just benchmarks\\n4. Fine-tuning: Consider fine-tuning for domain-specific tasks\\n6. Dimensionality: Don't assume more dimensions = better\\nFuture Directions\\nMultimodal Embeddings: Text, images, audio in shared space\\nLonger Context: Better embeddings for long documents\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:32+00:00', 'source': '../data/pdf/embeddings_explained.pdf', 'file_path': '../data/pdf/embeddings_explained.pdf', 'total_pages': 8, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:32+00:00', 'trapped': '', 'modDate': \"D:20260112090932+00'00'\", 'creationDate': \"D:20260112090932+00'00'\", 'page': 7}, page_content='embeddings_explained.md\\n2026-01-12\\n8 / 8\\nEfficiency: Smaller models with similar performance\\nBias Mitigation: Fairer, more equitable embeddings\\nSpecialized Models: Domain-specific embeddings\\nReal-time Learning: Embeddings that update with new data\\nConclusion\\nEmbeddings are fundamental to modern NLP and AI systems. They bridge the gap between human\\nlanguage and machine understanding, enabling:\\nSemantic search and retrieval\\nSimilarity computation\\nTransfer learning\\nEfficient representation of meaning\\nUnderstanding embeddings is crucial for building effective RAG systems, search engines, recommendation\\nsystems, and many other AI applications. As models continue to improve, embeddings will become even\\nmore powerful and versatile tools in the AI toolkit.\\nThe key is choosing the right embedding model for your specific use case, understanding its strengths and\\nlimitations, and continuously evaluating and improving your embedding-based systems.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:45+00:00', 'source': '../data/pdf/rag_explained.pdf', 'file_path': '../data/pdf/rag_explained.pdf', 'total_pages': 5, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:45+00:00', 'trapped': '', 'modDate': \"D:20260112090945+00'00'\", 'creationDate': \"D:20260112090945+00'00'\", 'page': 0}, page_content=\"rag_explained.md\\n2026-01-12\\n1 / 5\\nRetrieval-Augmented Generation (RAG) Explained\\nWhat is RAG?\\nRetrieval-Augmented Generation (RAG) is an advanced AI technique that combines the power of\\ninformation retrieval with generative language models. Instead of relying solely on a model's pre-trained\\nknowledge, RAG retrieves relevant information from external knowledge sources and uses it to generate\\nmore accurate, context-aware responses.\\nThe Problem RAG Solves\\nTraditional language models have several limitations:\\n-. Static Knowledge: They are trained on data up to a specific date and cannot access newer\\ninformation\\n/. Hallucinations: They may generate plausible-sounding but incorrect information\\n0. Limited Context: They cannot access domain-specific or proprietary knowledge\\n1. No Source Attribution: It's difficult to verify where information came from\\nRAG addresses these issues by grounding the model's responses in retrieved documents.\\nHow RAG Works\\nRAG operates in two main phases:\\nPhase 1: Retrieval\\n-. Document Processing: Convert documents into a searchable format\\nSplit documents into smaller chunks\\nGenerate embeddings (vector representations) for each chunk\\nStore embeddings in a vector database\\n/. Query Processing: When a user asks a question\\nConvert the query into an embedding\\nSearch the vector database for similar chunks\\nRetrieve the most relevant document chunks\\nPhase 2: Augmentation and Generation\\n-. Context Assembly: Combine the retrieved chunks with the user's query\\n/. Prompt Construction: Create a prompt that includes:\\nThe user's question\\nRetrieved relevant context\\nInstructions for the model\\n0. Generation: The language model generates a response based on the augmented context\\nRAG Architecture Components\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:45+00:00', 'source': '../data/pdf/rag_explained.pdf', 'file_path': '../data/pdf/rag_explained.pdf', 'total_pages': 5, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:45+00:00', 'trapped': '', 'modDate': \"D:20260112090945+00'00'\", 'creationDate': \"D:20260112090945+00'00'\", 'page': 1}, page_content='rag_explained.md\\n2026-01-12\\n2 / 5\\n1. Document Loader\\nExtracts text from various sources (PDFs, web pages, databases, etc.)\\nHandles different file formats and structures\\n2. Text Splitter\\nDivides documents into manageable chunks\\nBalances chunk size: too small loses context, too large is inefficient\\nCommon strategies: fixed-size, sentence-based, semantic chunking\\n3. Embedding Model\\nConverts text into numerical vectors (embeddings)\\nCaptures semantic meaning\\nEnables similarity search\\n4. Vector Database\\nStores document embeddings\\nEnables fast similarity search\\nExamples: Pinecone, Weaviate, Chroma, FAISS, Qdrant\\n5. Retrieval System\\nFinds relevant chunks based on query similarity\\nCan use various strategies:\\nDense retrieval (embedding similarity)\\nSparse retrieval (keyword matching)\\nHybrid approaches\\n6. Language Model\\nGenerates final responses\\nUses retrieved context to answer questions\\nCan cite sources from retrieved documents\\nRAG Workflow Example\\n1. User Query: \"What are the side effects of medication X?\" \\n    \\n2. Query Embedding: [0.23, -0.45, 0.67, ...] \\n    \\n3. Vector Search: Find top 5 most similar document chunks \\n   - Chunk 1: \"Medication X may cause dizziness...\" \\n   - Chunk 2: \"Common side effects include nausea...\" \\n   - Chunk 3: \"Patients should avoid alcohol...\" \\n    \\n4. Context Assembly: \\n   \"Based on the following information:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:45+00:00', 'source': '../data/pdf/rag_explained.pdf', 'file_path': '../data/pdf/rag_explained.pdf', 'total_pages': 5, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:45+00:00', 'trapped': '', 'modDate': \"D:20260112090945+00'00'\", 'creationDate': \"D:20260112090945+00'00'\", 'page': 2}, page_content='rag_explained.md\\n2026-01-12\\n3 / 5\\n   [Retrieved chunks] \\n    \\n   Answer: What are the side effects of medication X?\" \\n    \\n5. LLM Response: \"Based on the documentation, medication X  \\n   may cause dizziness, nausea, and patients should avoid  \\n   alcohol while taking it.\" \\nTypes of RAG\\n1. Naive RAG\\nBasic retrieval and generation\\nSimple similarity search\\nNo query optimization\\n2. Advanced RAG\\nQuery rewriting and expansion\\nRe-ranking retrieved results\\nMulti-step retrieval\\nQuery understanding\\n3. Modular RAG\\nComposable components\\nDifferent retrieval strategies\\nAdaptive retrieval methods\\nFine-tuned for specific domains\\nBenefits of RAG\\n-. Up-to-date Information: Can access current data without retraining\\n/. Reduced Hallucinations: Grounded in actual documents\\n0. Source Attribution: Can cite where information came from\\n1. Domain Expertise: Works with specialized knowledge bases\\n3. Cost Effective: More efficient than fine-tuning large models\\n4. Transparency: Users can verify sources\\nChallenges and Considerations\\n1. Retrieval Quality\\nPoor retrieval leads to irrelevant context\\nNeed good chunking strategies\\nRequires quality embeddings\\n2. Context Window Limits'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:45+00:00', 'source': '../data/pdf/rag_explained.pdf', 'file_path': '../data/pdf/rag_explained.pdf', 'total_pages': 5, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:45+00:00', 'trapped': '', 'modDate': \"D:20260112090945+00'00'\", 'creationDate': \"D:20260112090945+00'00'\", 'page': 3}, page_content='rag_explained.md\\n2026-01-12\\n4 / 5\\nLLMs have token limits\\nMust balance number of retrieved chunks\\nNeed to prioritize most relevant information\\n3. Chunking Strategy\\nToo small: loses context\\nToo large: includes irrelevant information\\nSemantic boundaries matter\\n4. Embedding Quality\\nDifferent models for different domains\\nMultilingual support\\nHandling specialized terminology\\n5. Query Understanding\\nUser queries may be ambiguous\\nNeed query expansion/rewriting\\nHandle synonyms and related concepts\\nBest Practices\\n-. Chunking: Use semantic chunking when possible, preserve context\\n/. Embeddings: Choose domain-appropriate embedding models\\n0. Retrieval: Use hybrid search (dense + sparse) for better results\\n1. Re-ranking: Re-rank retrieved results to improve relevance\\n3. Metadata: Store metadata (source, date, author) with chunks\\n4. Evaluation: Measure retrieval accuracy and response quality\\n6. Prompting: Design effective prompts that use retrieved context\\nRAG vs. Fine-tuning\\nAspect\\nRAG\\nFine-tuning\\nData Updates\\nEasy, just update database\\nRequires retraining\\nCost\\nLower, uses existing models\\nHigher, requires training\\nFlexibility\\nWorks with any domain\\nTied to training data\\nSource Attribution\\nYes\\nNo\\nImplementation\\nFaster to deploy\\nLonger development time\\nUse Cases\\n-. Question Answering Systems: Customer support, knowledge bases\\n/. Document Q&A: Legal documents, research papers, manuals\\n0. Chatbots: Enterprise chatbots with domain knowledge'),\n",
       " Document(metadata={'producer': 'Skia/PDF m80', 'creator': 'Chromium', 'creationdate': '2026-01-12T09:09:45+00:00', 'source': '../data/pdf/rag_explained.pdf', 'file_path': '../data/pdf/rag_explained.pdf', 'total_pages': 5, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2026-01-12T09:09:45+00:00', 'trapped': '', 'modDate': \"D:20260112090945+00'00'\", 'creationDate': \"D:20260112090945+00'00'\", 'page': 4}, page_content='rag_explained.md\\n2026-01-12\\n5 / 5\\n1. Code Assistance: Documentation-based coding help\\n3. Research Tools: Academic paper analysis\\n4. Content Generation: Blog posts, reports based on sources\\nFuture Directions\\nMulti-modal RAG: Incorporating images, tables, and other media\\nActive Learning: Improving retrieval through user feedback\\nReal-time Updates: Streaming new information into knowledge bases\\nBetter Evaluation: More sophisticated metrics for RAG systems\\nHybrid Architectures: Combining RAG with fine-tuning\\nConclusion\\nRAG represents a powerful approach to building AI systems that can leverage external knowledge\\neffectively. By combining retrieval and generation, RAG systems can provide accurate, up-to-date, and\\nverifiable responses while maintaining the flexibility and natural language capabilities of modern LLMs.\\nThe key to successful RAG implementation lies in:\\nQuality document processing and chunking\\nEffective embedding and retrieval strategies\\nCareful prompt engineering\\nContinuous evaluation and improvement\\nAs the field evolves, RAG will continue to be a cornerstone of practical AI applications that need to work\\nwith real-world, dynamic knowledge sources.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PDF Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f58319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "411f220c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/yashkalra/Documents/Projects/agentic-ai-learning-lab/YTRAG/data/pdf/rag_explained.html', 'title': 'rag_explained.md'}, page_content='\\n\\n\\nrag_explained.md\\n\\n\\n\\n\\n\\n\\n\\n\\nRetrieval-Augmented Generation (RAG) Explained\\nWhat is RAG?\\nRetrieval-Augmented Generation (RAG) is an advanced AI technique that combines the power of information retrieval with generative language models. Instead of relying solely on a model\\'s pre-trained knowledge, RAG retrieves relevant information from external knowledge sources and uses it to generate more accurate, context-aware responses.\\nThe Problem RAG Solves\\nTraditional language models have several limitations:\\n\\nStatic Knowledge: They are trained on data up to a specific date and cannot access newer information\\nHallucinations: They may generate plausible-sounding but incorrect information\\nLimited Context: They cannot access domain-specific or proprietary knowledge\\nNo Source Attribution: It\\'s difficult to verify where information came from\\n\\nRAG addresses these issues by grounding the model\\'s responses in retrieved documents.\\nHow RAG Works\\nRAG operates in two main phases:\\nPhase 1: Retrieval\\n\\n\\nDocument Processing: Convert documents into a searchable format\\n\\nSplit documents into smaller chunks\\nGenerate embeddings (vector representations) for each chunk\\nStore embeddings in a vector database\\n\\n\\n\\nQuery Processing: When a user asks a question\\n\\nConvert the query into an embedding\\nSearch the vector database for similar chunks\\nRetrieve the most relevant document chunks\\n\\n\\n\\nPhase 2: Augmentation and Generation\\n\\nContext Assembly: Combine the retrieved chunks with the user\\'s query\\nPrompt Construction: Create a prompt that includes:\\n\\nThe user\\'s question\\nRetrieved relevant context\\nInstructions for the model\\n\\n\\nGeneration: The language model generates a response based on the augmented context\\n\\nRAG Architecture Components\\n1. Document Loader\\n\\nExtracts text from various sources (PDFs, web pages, databases, etc.)\\nHandles different file formats and structures\\n\\n2. Text Splitter\\n\\nDivides documents into manageable chunks\\nBalances chunk size: too small loses context, too large is inefficient\\nCommon strategies: fixed-size, sentence-based, semantic chunking\\n\\n3. Embedding Model\\n\\nConverts text into numerical vectors (embeddings)\\nCaptures semantic meaning\\nEnables similarity search\\n\\n4. Vector Database\\n\\nStores document embeddings\\nEnables fast similarity search\\nExamples: Pinecone, Weaviate, Chroma, FAISS, Qdrant\\n\\n5. Retrieval System\\n\\nFinds relevant chunks based on query similarity\\nCan use various strategies:\\n\\nDense retrieval (embedding similarity)\\nSparse retrieval (keyword matching)\\nHybrid approaches\\n\\n\\n\\n6. Language Model\\n\\nGenerates final responses\\nUses retrieved context to answer questions\\nCan cite sources from retrieved documents\\n\\nRAG Workflow Example\\n1. User Query: \"What are the side effects of medication X?\"\\n   \\n2. Query Embedding: [0.23, -0.45, 0.67, ...]\\n   \\n3. Vector Search: Find top 5 most similar document chunks\\n   - Chunk 1: \"Medication X may cause dizziness...\"\\n   - Chunk 2: \"Common side effects include nausea...\"\\n   - Chunk 3: \"Patients should avoid alcohol...\"\\n   \\n4. Context Assembly:\\n   \"Based on the following information:\\n   [Retrieved chunks]\\n   \\n   Answer: What are the side effects of medication X?\"\\n   \\n5. LLM Response: \"Based on the documentation, medication X \\n   may cause dizziness, nausea, and patients should avoid \\n   alcohol while taking it.\"\\n\\nTypes of RAG\\n1. Naive RAG\\n\\nBasic retrieval and generation\\nSimple similarity search\\nNo query optimization\\n\\n2. Advanced RAG\\n\\nQuery rewriting and expansion\\nRe-ranking retrieved results\\nMulti-step retrieval\\nQuery understanding\\n\\n3. Modular RAG\\n\\nComposable components\\nDifferent retrieval strategies\\nAdaptive retrieval methods\\nFine-tuned for specific domains\\n\\nBenefits of RAG\\n\\nUp-to-date Information: Can access current data without retraining\\nReduced Hallucinations: Grounded in actual documents\\nSource Attribution: Can cite where information came from\\nDomain Expertise: Works with specialized knowledge bases\\nCost Effective: More efficient than fine-tuning large models\\nTransparency: Users can verify sources\\n\\nChallenges and Considerations\\n1. Retrieval Quality\\n\\nPoor retrieval leads to irrelevant context\\nNeed good chunking strategies\\nRequires quality embeddings\\n\\n2. Context Window Limits\\n\\nLLMs have token limits\\nMust balance number of retrieved chunks\\nNeed to prioritize most relevant information\\n\\n3. Chunking Strategy\\n\\nToo small: loses context\\nToo large: includes irrelevant information\\nSemantic boundaries matter\\n\\n4. Embedding Quality\\n\\nDifferent models for different domains\\nMultilingual support\\nHandling specialized terminology\\n\\n5. Query Understanding\\n\\nUser queries may be ambiguous\\nNeed query expansion/rewriting\\nHandle synonyms and related concepts\\n\\nBest Practices\\n\\nChunking: Use semantic chunking when possible, preserve context\\nEmbeddings: Choose domain-appropriate embedding models\\nRetrieval: Use hybrid search (dense + sparse) for better results\\nRe-ranking: Re-rank retrieved results to improve relevance\\nMetadata: Store metadata (source, date, author) with chunks\\nEvaluation: Measure retrieval accuracy and response quality\\nPrompting: Design effective prompts that use retrieved context\\n\\nRAG vs. Fine-tuning\\n\\n\\n\\nAspect\\nRAG\\nFine-tuning\\n\\n\\n\\n\\nData Updates\\nEasy, just update database\\nRequires retraining\\n\\n\\nCost\\nLower, uses existing models\\nHigher, requires training\\n\\n\\nFlexibility\\nWorks with any domain\\nTied to training data\\n\\n\\nSource Attribution\\nYes\\nNo\\n\\n\\nImplementation\\nFaster to deploy\\nLonger development time\\n\\n\\n\\nUse Cases\\n\\nQuestion Answering Systems: Customer support, knowledge bases\\nDocument Q&A: Legal documents, research papers, manuals\\nChatbots: Enterprise chatbots with domain knowledge\\nCode Assistance: Documentation-based coding help\\nResearch Tools: Academic paper analysis\\nContent Generation: Blog posts, reports based on sources\\n\\nFuture Directions\\n\\nMulti-modal RAG: Incorporating images, tables, and other media\\nActive Learning: Improving retrieval through user feedback\\nReal-time Updates: Streaming new information into knowledge bases\\nBetter Evaluation: More sophisticated metrics for RAG systems\\nHybrid Architectures: Combining RAG with fine-tuning\\n\\nConclusion\\nRAG represents a powerful approach to building AI systems that can leverage external knowledge effectively. By combining retrieval and generation, RAG systems can provide accurate, up-to-date, and verifiable responses while maintaining the flexibility and natural language capabilities of modern LLMs.\\nThe key to successful RAG implementation lies in:\\n\\nQuality document processing and chunking\\nEffective embedding and retrieval strategies\\nCareful prompt engineering\\nContinuous evaluation and improvement\\n\\nAs the field evolves, RAG will continue to be a cornerstone of practical AI applications that need to work with real-world, dynamic knowledge sources.\\n\\n\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HTML Loader\n",
    "loader = BSHTMLLoader(\n",
    "    \"/Users/yashkalra/Documents/Projects/agentic-ai-learning-lab/YTRAG/data/pdf/rag_explained.html\",\n",
    "    bs_kwargs={\"features\": \"html.parser\"}\n",
    ")\n",
    "documents = loader.load()\n",
    "documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beac7fc",
   "metadata": {},
   "source": [
    "### Embedding & VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f2b9cbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Settings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c390a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mall-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m emb = model.encode(\u001b[33m\"\u001b[39m\u001b[33mRAG is awesome\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(emb))\n",
      "\u001b[31mNameError\u001b[39m: name 'SentenceTransformer' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "emb = model.encode(\"RAG is awesome\")\n",
    "print(len(emb))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
